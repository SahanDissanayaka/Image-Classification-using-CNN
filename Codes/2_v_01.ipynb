{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './realwaste/realwaste-main/RealWaste'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_partitions(ds, train_split=0.6, val_split=0.2, test_split=0.2, shuffle=True, shuffle_size = 10000):\n",
    "\n",
    "    ds_size = len(ds)\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "\n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "\n",
    "    train_ds = ds.take(train_size)\n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "\n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4752 files belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from a directory (adjust path)\n",
    "dataset_path = './realwaste/realwaste-main/RealWaste'\n",
    "\n",
    "# Use TensorFlow's image_dataset_from_directory to load the dataset\n",
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    image_size=(524, 524),  # Resize all images to 128x128\n",
    "    batch_size=32,          # Batch size for loading\n",
    "    label_mode='int'        # Use integer labels\n",
    ")\n",
    "\n",
    "# Partition the dataset into training, validation, and test sets\n",
    "train_ds, val_ds, test_ds = get_dataset_partitions(dataset, train_split=0.6, val_split=0.2, test_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE  # Auto-tune for performance optimization\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize image to [0, 1]\n",
    "    image = tf.image.resize(image, [128, 128])  # Resize image to 128x128\n",
    "    return image, label\n",
    "\n",
    "# Apply preprocessing\n",
    "train_ds = train_ds.map(preprocess).cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.map(preprocess).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.map(preprocess).cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ResNet50 model with ImageNet weights (excluding the top classification layers)\n",
    "resnet_base = tf.keras.applications.ResNet50(\n",
    "    include_top=False,          # Exclude top layers (we'll add our own)\n",
    "    weights='imagenet',         # Load pre-trained weights from ImageNet\n",
    "    input_shape=(128, 128, 3)   # Input size (128x128 images with 3 channels)\n",
    ")\n",
    "\n",
    "# Freeze the layers of ResNet50 to prevent updating during initial training\n",
    "resnet_base.trainable = False\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Build the model using tf.keras\n",
    "model = models.Sequential([\n",
    "    resnet_base,  # Add pre-trained ResNet50 as the base model\n",
    "    layers.GlobalAveragePooling2D(),  # Pool feature maps into a 1D vector\n",
    "    layers.Dense(128, activation='relu'),  # Fully connected layer\n",
    "    layers.Dropout(0.5),  # Dropout layer\n",
    "    layers.Dense(len(dataset.class_names), activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',  # Use sparse categorical crossentropy for integer labels\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 13:52:11.176173: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 105437312 bytes after encountering the first element of size 105437312 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    }
   ],
   "source": [
    "class CustomTrainingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_loss = logs.get('loss', 0)\n",
    "        train_accuracy = logs.get('accuracy', 0) * 100  # Convert to percentage\n",
    "        val_loss = logs.get('val_loss', 0)\n",
    "        val_accuracy = logs.get('val_accuracy', 0) * 100  # Convert to percentage\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{self.params['epochs']}, \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}% \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Assuming `model`, `train_ds`, and `val_ds` are already defined\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=20,  # number of epochs\n",
    "    verbose=0,  \n",
    "    callbacks=[CustomTrainingCallback()] \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
